### CNN

1×1的卷积的作用：步长不小于2，取代pooling，有resize的作用

#### relu非线性，将线性变成非线性

非线性有什么用： 拟合复杂系统

为什么要非线性：因为没有非线性的话，全连接层的输出只是多个参数相乘相加，不管有多少层，都只是线性的关系。

sigmod的缺点：它在左边后右边的梯度基本接近于0，会导致梯度消失，中间部分的梯度基本接近于线性，容易梯度爆炸，它不能用处非最终层的layer上，因为当某个输出0.8，那0.8乘了很多次，就变成很多0 的0.0000008，这就很接近0了，用relu的话，它会变1，多次相乘还是1，就不会出现梯度消失的问题。

#### 池化

最大池化，平均池化，减少了维度，减少了tensor size

#### dropout

* 减少参数

* 集成效果

  训练的时候，会以概率P选择神经原训练，剩下的不进行训练，而每次拿这些训练后下一次先恢复被屏蔽掉的神经元，再以p选择神经元继续训练，每次反向传播更新的参数也只是每次进行训练了的那部分参数。

  训练的时候会以p概率选择，为了与测试保持一致，因为测试的时候不进行训练了，但是我们要保持神经元的一致，但我们也没法知道每一次都扔掉了哪些神经元，所以如果训练的时候没有rescale的话，测试的时候就要乘概率p，但为了加快测试速度，一般是在训练的时候训练完后除以p来保持前后一致。

  集成效果：每次只选择概率p的神经原训练，每次选择的都是不一样的，相当于我们训练了很多个不一样的网络，就起到了集成的效果。

#### BN

BN的过程就是，因为将图像经过方差为1均值为0 的标准化操作后生成新的图像，然后对这些图像进行卷积，鉴于sigmod中间有像线性的梯度，而线性的梯度对结果表现不好，会丧失拟合复杂系统的能力，所以会继续将卷积完的图像**映射**回原图像。

通常BN在网络中的位置：

Conv -> BN -> Relu -> Pool

优点：减少了过拟合，不易过拟合，这样的话就可以减少其他防止过拟合的方式，如可以减少正则化权重，少用了图像增强的方法，少用dropout。

出现过拟合一般就是在数据集较为边缘的地方，标准化的方式就是将边缘变得平滑，防止出现一些容易出现过拟合的数据。

训练的时候是对每个batch计算mean 和val，测试的时候是用训练最后一个batch的mean 和val。

> 实际上就是为了防止过拟合，将其标准化以后，去做卷积，但为了防止其丧失拟合复杂系统的能力，模型过于简单了。就再映射回去，这样就更少出现过拟合现象。