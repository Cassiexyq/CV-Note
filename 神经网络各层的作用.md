### CNN

1×1的卷积的作用：步长不小于2，取代pooling，有resize的作用

#### relu非线性，将线性变成非线性

非线性有什么用： 拟合复杂系统

为什么要非线性：因为没有非线性的话，全连接层的输出只是多个参数相乘相加，不管有多少层，都只是线性的关系。如果不使用激活函数，这种情况下每一层输出都是上一层输入的线性函数。无论神经网络有多少层，输出都是输入的线性函数，这样就和只有一个隐藏层的效果是一样的。这种情况相当于多层感知机(MLP)

relu的拐点问题

sigmod的缺点：它在左边后右边的梯度基本接近于0，会导致梯度消失，中间部分的梯度基本接近于线性，容易梯度爆炸，它不能用处非最终层的layer上，因为当某个输出0.8，那0.8乘了很多次，就变成很多0 的0.0000008，这就很接近0了，用relu的话，它会变1，多次相乘还是1，就不会出现梯度消失的问题。

#### 池化

最大池化，平均池化，减少了维度，减少了tensor size

#### dropout

* 减少参数

* 集成效果

  训练的时候，会以概率P选择神经原训练，剩下的不进行训练，而每次拿这些训练后下一次先恢复被屏蔽掉的神经元，再以p选择神经元继续训练，每次反向传播更新的参数也只是每次进行训练了的那部分参数。

  训练的时候会以p概率选择，为了与测试保持一致，因为测试的时候不进行训练了，但是我们要保持神经元的一致，但我们也没法知道每一次都扔掉了哪些神经元，所以如果训练的时候没有rescale的话，测试的时候就要乘概率p，但为了加快测试速度，一般是在训练的时候训练完后除以p来保持前后一致。

  集成效果：每次只选择概率p的神经原训练，每次选择的都是不一样的，相当于我们训练了很多个不一样的网络，就起到了集成的效果。

#### BN

BN的过程就是，因为将图像经过方差为1均值为0 的标准化操作后生成新的图像，然后对这些图像进行卷积，鉴于sigmod中间有像线性的梯度，而线性的梯度对结果表现不好，会丧失拟合复杂系统的能力，所以会继续将卷积完的图像**映射**回原图像。

通常BN在网络中的位置：

Conv -> BN -> Relu -> Pool

优点：减少了过拟合，不易过拟合，这样的话就可以减少其他防止过拟合的方式，如可以减少正则化权重，少用了图像增强的方法，少用dropout。

出现过拟合一般就是在数据集较为边缘的地方，标准化的方式就是将边缘变得平滑，防止出现一些容易出现过拟合的数据。

训练的时候是对每个batch计算mean 和val，测试的时候是用训练最后一个batch的mean 和val。

> 实际上就是为了防止过拟合，将其标准化以后，去做卷积，但为了防止其丧失拟合复杂系统的能力，模型过于简单了。就再映射回去，这样就更少出现过拟合现象。



**GAP**

每个讲到全局池化的都会说GAP就是把avg pooling的窗口大小设置成feature map的大小，这虽然是正确的，但这并不是GAP内涵的全部。GAP的意义是对整个网络从结构上做正则化防止过拟合**。既要参数少避免全连接带来的过拟合风险，又要能达到全连接一样的转换功能**，怎么做呢？直接从feature map的通道上下手，如果我们最终有1000类，那么最后一层卷积输出的feature map就只有1000个channel，然后对这个feature map应用全局池化，输出长度为1000的向量，这就相当于剔除了全连接层黑箱子操作的特征，直接赋予了每个channel实际的类别意义。

Full, Same ,valid

为什么神经元要有激活函数

1. 再多层也是单层感知机
2. 神经网络的实际的数学作用是拟合实际函数