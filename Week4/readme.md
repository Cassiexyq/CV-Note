## L1 L2 

#### 正则

L1正则化结果容易产生0值，是一个稀疏矩阵，有助于特征选择，只关注非0值

L2正则化有利于防止过拟合，倾向于权值尽可能小，因为参数值小的莫i选哪个比较简单，能适应不同的而数据集

$\theta_j := \theta_j(1-\frac{\lambda}{m}) - \alpha\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^i)-y^i)x_j^i)$

可以看到$\theta_j$ 要剪掉一个小于1的因子，从而使得$\theta_j$ 不断变小，整个总的来说变小的幅度更快了

#### loss

L1 loss: 绝对损失函数 -》 Fast RCNN      

$L1 = |f(x) -Y|$

L2 loss: 平方损失函数 -》 边框预测回归

$L2 = (f(x) -Y)^2$

smooth L1 loss 让loss对于离群点更加鲁棒，相比于L2，其对损离群点，异常值不敏感，梯度变化相对更小

L2 缺点： 因为平方增长的缘故，当存在离群点时，这些点会占loss主要部分。 

